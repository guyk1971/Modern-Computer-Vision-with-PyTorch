{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PacktPublishing/Modern-Computer-Vision-with-PyTorch/blob/master/Chapter05/VGG_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding VGG16 architecture\n",
    "VGG stands for Visual Geometry Group, which is based out of the University of Oxford, and 16 stands for the number of layers in the model.  \n",
    "The VGG16 model is trained to classify objects in the ImageNet competition and stood as the runner-up architecture in 2014.  \n",
    "The reason we are studying this architecture instead of the winning architecture (GoogleNet) is because of its simplicity and a larger acceptance in the vision community by using it in several other tasks.  \n",
    "Let's understand the architecture of VGG16 along with how a VGG16 pre-trained model is accessible and represented in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T19:31:58.697983Z",
     "start_time": "2021-11-16T19:31:58.085572Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGFMlprVNhM9",
    "outputId": "87d633f5-43a1-4f2b-b1f9-cd0c50d88553"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms,models,datasets\n",
    "# !pip install torch_summary\n",
    "from torchsummary import summary\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T19:33:21.251975Z",
     "start_time": "2021-11-16T19:32:03.455646Z"
    },
    "id": "3VRuWU7rNjNq"
   },
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vgg arch](./vgg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T19:33:40.890877Z",
     "start_time": "2021-11-16T19:33:40.659103Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQ9BIH_QNkyY",
    "outputId": "f7f6af55-29b7-4df6-8974-3e80abd635f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 512, 7, 7]           --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 224, 224]        1,792\n",
      "|    └─ReLU: 2-2                         [-1, 64, 224, 224]        --\n",
      "|    └─Conv2d: 2-3                       [-1, 64, 224, 224]        36,928\n",
      "|    └─ReLU: 2-4                         [-1, 64, 224, 224]        --\n",
      "|    └─MaxPool2d: 2-5                    [-1, 64, 112, 112]        --\n",
      "|    └─Conv2d: 2-6                       [-1, 128, 112, 112]       73,856\n",
      "|    └─ReLU: 2-7                         [-1, 128, 112, 112]       --\n",
      "|    └─Conv2d: 2-8                       [-1, 128, 112, 112]       147,584\n",
      "|    └─ReLU: 2-9                         [-1, 128, 112, 112]       --\n",
      "|    └─MaxPool2d: 2-10                   [-1, 128, 56, 56]         --\n",
      "|    └─Conv2d: 2-11                      [-1, 256, 56, 56]         295,168\n",
      "|    └─ReLU: 2-12                        [-1, 256, 56, 56]         --\n",
      "|    └─Conv2d: 2-13                      [-1, 256, 56, 56]         590,080\n",
      "|    └─ReLU: 2-14                        [-1, 256, 56, 56]         --\n",
      "|    └─Conv2d: 2-15                      [-1, 256, 56, 56]         590,080\n",
      "|    └─ReLU: 2-16                        [-1, 256, 56, 56]         --\n",
      "|    └─MaxPool2d: 2-17                   [-1, 256, 28, 28]         --\n",
      "|    └─Conv2d: 2-18                      [-1, 512, 28, 28]         1,180,160\n",
      "|    └─ReLU: 2-19                        [-1, 512, 28, 28]         --\n",
      "|    └─Conv2d: 2-20                      [-1, 512, 28, 28]         2,359,808\n",
      "|    └─ReLU: 2-21                        [-1, 512, 28, 28]         --\n",
      "|    └─Conv2d: 2-22                      [-1, 512, 28, 28]         2,359,808\n",
      "|    └─ReLU: 2-23                        [-1, 512, 28, 28]         --\n",
      "|    └─MaxPool2d: 2-24                   [-1, 512, 14, 14]         --\n",
      "|    └─Conv2d: 2-25                      [-1, 512, 14, 14]         2,359,808\n",
      "|    └─ReLU: 2-26                        [-1, 512, 14, 14]         --\n",
      "|    └─Conv2d: 2-27                      [-1, 512, 14, 14]         2,359,808\n",
      "|    └─ReLU: 2-28                        [-1, 512, 14, 14]         --\n",
      "|    └─Conv2d: 2-29                      [-1, 512, 14, 14]         2,359,808\n",
      "|    └─ReLU: 2-30                        [-1, 512, 14, 14]         --\n",
      "|    └─MaxPool2d: 2-31                   [-1, 512, 7, 7]           --\n",
      "├─AdaptiveAvgPool2d: 1-2                 [-1, 512, 7, 7]           --\n",
      "├─Sequential: 1-3                        [-1, 1000]                --\n",
      "|    └─Linear: 2-32                      [-1, 4096]                102,764,544\n",
      "|    └─ReLU: 2-33                        [-1, 4096]                --\n",
      "|    └─Dropout: 2-34                     [-1, 4096]                --\n",
      "|    └─Linear: 2-35                      [-1, 4096]                16,781,312\n",
      "|    └─ReLU: 2-36                        [-1, 4096]                --\n",
      "|    └─Dropout: 2-37                     [-1, 4096]                --\n",
      "|    └─Linear: 2-38                      [-1, 1000]                4,097,000\n",
      "==========================================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 15.61\n",
      "==========================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 103.43\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 631.80\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary(model, torch.zeros(1,3,224,224));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T19:34:06.448632Z",
     "start_time": "2021-11-16T19:34:06.422269Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StQBD486NmM-",
    "outputId": "612481e0-7665-4b5c-9c5a-ebf76f335178"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are three major sub-modules in the model—features, avgpool, and classifier.  \n",
    "Typically, we would freeze the features and avgpool modules. Delete the classifier module (or only a few layers at the bottom) and create a new one in its place that will predict the required number of classes corresponding to our\n",
    "dataset (instead of the existing 1,000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Let's understand how the VGG16 model is used in practice, using the cats versus dogs dataset (considering only 500 images in each class for training) in the [following code](./Implementing_VGG16_for_image_classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TBnhG2vTNr8R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPe/3ZXN0rsAfvPzmbfIYYW",
   "include_colab_link": true,
   "name": "VGG_architecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
