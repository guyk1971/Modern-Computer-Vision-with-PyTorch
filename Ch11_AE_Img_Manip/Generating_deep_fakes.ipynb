{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PacktPublishing/Hands-On-Computer-Vision-with-PyTorch/blob/master/Chapter11/Generating_deep_fakes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating deep fake\n",
    "Imagine a scenario where you want to create an application that takes a given image of a face and changes the facial expression in a way that you want. Deep fakes come in handy in this scenario. While we will not discuss the very latest in deep fakes, techniques such as few-shot adversarial learning are developed to generate realistic images with the facial expression of interest. Knowledge of how deep fakes work and GANs.  \n",
    "\n",
    "In the task of deep fakery, we would have a few hundred pictures of person A and a few hundred pictures of person B. The objective is to reconstruct person B's face with the facial expression of person A and vice versa.  \n",
    "\n",
    "to understand how it works, lets look at the following image:  \n",
    "![img1](Ch11_deepfake1.png)  \n",
    "\n",
    "In the preceding picture, we are passing images of person A and person B through an encoder (Encoder). Once we get the latent vectors corresponding to person A (Latent Face A) and person B (Latent Face B), we pass the latent vectors through their corresponding decoders (Decoder A and Decoder B) to fetch the corresponding original images (Reconstructed Face A and Reconstructed Face B). So far, the concept of encoder and decoder is very similar to what we learned in the Autoencoders section. However, in this scenario, we have only one encoder, but two decoders (each decoder corresponding to a different person). The expectation is that the latent vectors obtained from the encoder represent the information about the facial expression present within the image, while the decoder fetches the image corresponding to the person. Once the encoder and the two decoders are trained, while performing deep fake image generation, we switch the connection within our architecture as follows:  \n",
    "![img2](Ch11_deepfake2.png)  \n",
    "\n",
    "When the latent vector of person A is passed through decoder B, the reconstructed face of person B will have the characteristics of person A (a smiling face) and vice versa for person B when passed through decoder A (a sad face).  \n",
    "\n",
    "Note: One additional trick that helps in generating a realistic image is warping face images and feeding them to the network in such a way that the warped face is the input and the original image is expected as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "ov13_44WSBQ8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('Faceswap-Deepfake-Pytorch'):\n",
    "    !wget -q https://www.dropbox.com/s/5ji7jl7httso9ny/person_images.zip\n",
    "    !wget -q https://raw.githubusercontent.com/sizhky/deep-fake-util/main/random_warp.py\n",
    "    !unzip -q person_images.zip\n",
    "# !pip install -q torch_snippets torch_summary\n",
    "from torch_snippets import *\n",
    "from random_warp import get_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_RsQrIKSBRB"
   },
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "CtZ88vcGSBRE"
   },
   "outputs": [],
   "source": [
    "def crop_face(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    if(len(faces)>0):\n",
    "        for (x,y,w,h) in faces:\n",
    "            img2 = img[y:(y+h),x:(x+w),:]\n",
    "        img2 = cv2.resize(img2,(256,256))\n",
    "        return img2, True\n",
    "    else:\n",
    "        return img, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZxVqeKsSBRH",
    "outputId": "ac12ddf4-3317-44f7-f761-9769eabe428e"
   },
   "outputs": [],
   "source": [
    "!mkdir cropped_faces_personA\n",
    "!mkdir cropped_faces_personB\n",
    "\n",
    "def crop_images(folder):\n",
    "    images = Glob(folder+'/*.jpg')\n",
    "    for i in range(len(images)):\n",
    "        img = read(images[i],1)\n",
    "        img2, face_detected = crop_face(img)\n",
    "        if(face_detected==False):\n",
    "            continue\n",
    "        else:\n",
    "            cv2.imwrite('cropped_faces_'+folder+'/'+str(i)+'.jpg',cv2.cvtColor(img2, cv2.COLOR_RGB2BGR))\n",
    "crop_images('personA')\n",
    "crop_images('personB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoxzhIRwSBRK",
    "outputId": "ccb817be-b455-4dca-8c6e-7ab6f8e479f1"
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, items_A, items_B):\n",
    "        self.items_A = np.concatenate([read(f,1)[None] for f in items_A])/255.\n",
    "        self.items_B = np.concatenate([read(f,1)[None] for f in items_B])/255.\n",
    "        self.items_A += self.items_B.mean(axis=(0, 1, 2)) - self.items_A.mean(axis=(0, 1, 2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.items_A), len(self.items_B))\n",
    "    def __getitem__(self, ix):\n",
    "        a, b = choose(self.items_A), choose(self.items_B)\n",
    "        return a, b\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        imsA, imsB = list(zip(*batch))\n",
    "        imsA, targetA = get_training_data(imsA, len(imsA))\n",
    "        imsB, targetB = get_training_data(imsB, len(imsB))\n",
    "        imsA, imsB, targetA, targetB = [torch.Tensor(i).permute(0,3,1,2).to(device) for i in [imsA, imsB, targetA, targetB]]\n",
    "        return imsA, imsB, targetA, targetB\n",
    "\n",
    "a = ImageDataset(Glob('cropped_faces_personA'), Glob('cropped_faces_personB'))\n",
    "x = DataLoader(a, batch_size=32, collate_fn=a.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ontuFTvOSBRQ",
    "outputId": "7726c63f-b1cd-4e86-f115-816572a41356"
   },
   "outputs": [],
   "source": [
    "inspect(*next(iter(x)))\n",
    "\n",
    "for i in next(iter(x)):\n",
    "    subplots(i[:8], nc=4, sz=(4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDGF47-xSBRT"
   },
   "outputs": [],
   "source": [
    "def _ConvLayer(input_features, output_features):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(input_features, output_features, kernel_size=5, stride=2, padding=2),\n",
    "        nn.LeakyReLU(0.1, inplace=True)\n",
    "    )\n",
    "\n",
    "def _UpScale(input_features, output_features):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(input_features, output_features, kernel_size=2, stride=2, padding=0),\n",
    "        nn.LeakyReLU(0.1, inplace=True)\n",
    "    )\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def forward(self, input):\n",
    "        output = input.view(-1, 1024, 4, 4) # channel * 4 * 4\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwPNNemnSBRW"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            _ConvLayer(3, 128),\n",
    "            _ConvLayer(128, 256),\n",
    "            _ConvLayer(256, 512),\n",
    "            _ConvLayer(512, 1024),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * 4 * 4, 1024),\n",
    "            nn.Linear(1024, 1024 * 4 * 4),\n",
    "            Reshape(),\n",
    "            _UpScale(1024, 512),\n",
    "        )\n",
    "\n",
    "        self.decoder_A = nn.Sequential(\n",
    "            _UpScale(512, 256),\n",
    "            _UpScale(256, 128),\n",
    "            _UpScale(128, 64),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.decoder_B = nn.Sequential(\n",
    "            _UpScale(512, 256),\n",
    "            _UpScale(256, 128),\n",
    "            _UpScale(128, 64),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, select='A'):\n",
    "        if select == 'A':\n",
    "            out = self.encoder(x)\n",
    "            out = self.decoder_A(out)\n",
    "        else:\n",
    "            out = self.encoder(x)\n",
    "            out = self.decoder_B(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvKnOm2SSBRY",
    "outputId": "879f6308-f665-4385-b732-1a9cbf945e40"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "model = Autoencoder()\n",
    "summary(model, torch.zeros(32,3,64,64), 'A');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gY_vgx2dSBRb"
   },
   "outputs": [],
   "source": [
    "def train_batch(model, data, criterion, optimizes):\n",
    "    optA, optB = optimizers\n",
    "    optA.zero_grad()\n",
    "    optB.zero_grad()\n",
    "    imgA, imgB, targetA, targetB = data\n",
    "    _imgA, _imgB = model(imgA, 'A'), model(imgB, 'B')\n",
    "\n",
    "    lossA = criterion(_imgA, targetA)\n",
    "    lossB = criterion(_imgB, targetB)\n",
    "    \n",
    "    lossA.backward()\n",
    "    lossB.backward()\n",
    "\n",
    "    optA.step()\n",
    "    optB.step()\n",
    "\n",
    "    return lossA.item(), lossB.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tg_gWHRUSBRe",
    "outputId": "53203e83-bd9f-479d-b6ba-88db4526f26a"
   },
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "\n",
    "dataset = ImageDataset(Glob('cropped_faces_personA'), Glob('cropped_faces_personB'))\n",
    "dataloader = DataLoader(dataset, 32, collate_fn=dataset.collate_fn)\n",
    "\n",
    "optimizers = optim.Adam([{'params': model.encoder.parameters()},\n",
    "                          {'params': model.decoder_A.parameters()}],\n",
    "                        lr=5e-5, betas=(0.5, 0.999)), \\\n",
    "             optim.Adam([{'params': model.encoder.parameters()},\n",
    "                          {'params': model.decoder_B.parameters()}], \n",
    "                        lr=5e-5, betas=(0.5, 0.999))\n",
    "             \n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXfA-mSuSBRg",
    "outputId": "f08c5cc3-08eb-49ae-d9da-e615611d2547"
   },
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "log = Report(n_epochs)\n",
    "!mkdir checkpoint\n",
    "for ex in range(n_epochs):\n",
    "    N = len(dataloader)\n",
    "    for bx,data in enumerate(dataloader):\n",
    "        lossA, lossB = train_batch(model, data, criterion, optimizers)\n",
    "        log.record(ex+(1+bx)/N, lossA=lossA, lossB=lossB, end='\\r')\n",
    "\n",
    "    log.report_avgs(ex+1)\n",
    "    if (ex+1)%100 == 0:\n",
    "        state = {\n",
    "                'state': model.state_dict(),\n",
    "                'epoch': ex\n",
    "            }\n",
    "        torch.save(state, './checkpoint/autoencoder.pth')\n",
    "\n",
    "    if (ex+1)%100 == 0:\n",
    "        bs = 5\n",
    "        a,b,A,B = data\n",
    "        line('A to B')\n",
    "        _a = model(a[:bs], 'A')\n",
    "        _b = model(a[:bs], 'B')\n",
    "        x = torch.cat([A[:bs],_a,_b])\n",
    "        subplots(x, nc=bs, figsize=(bs*2, 5))\n",
    "\n",
    "        line('B to A')\n",
    "        _a = model(b[:bs], 'A')\n",
    "        _b = model(b[:bs], 'B')\n",
    "        x = torch.cat([B[:bs],_a,_b])\n",
    "        subplots(x, nc=bs, figsize=(bs*2, 5))\n",
    "\n",
    "log.plot_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKyTucOGSBRi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Generating deep fakes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "941b857050c13d07454f805cfe65e1adb2a007f4f605236f8222b1570c40ffba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
