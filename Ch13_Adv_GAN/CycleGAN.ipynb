{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PacktPublishing/Hands-On-Computer-Vision-with-PyTorch/blob/master/Chapter13/CycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle GAN\n",
    "The goal of cycle GAN is to convert an image of one object (e.g. apple) to an image of another object (e.g. orange).\n",
    "we're not given with a pair of input and output images - but get the images of both classes in two distinct folders.\n",
    "\n",
    "what we want to do is to convert image from one class to another and back to the original class.\n",
    "\n",
    "The architecture have 3 loss values:\n",
    "- Discriminator loss: This ensures that the object class is modified while training the model (as seen in the previous section).\n",
    "- Cycle loss: The loss of recycling an image from the generated image to the original to ensure that the surrounding pixels are not changed.\n",
    "- Identity loss: The loss when an image of one class is passed through a generator that is expected to convert an image of another class into the class of the input image.  \n",
    "\n",
    "The steps for building the cycle GAN is as follows:\n",
    "1. Import and preprocess the dataset\n",
    "2. Build the generator and discriminator network UNet architectures\n",
    "3. Define two generators:\n",
    "    - G_AB: Generator that converts an image of class A to an image of class B \n",
    "    - G_BA: Generator that converts an image of class B to an image of class A  \n",
    "    \n",
    "    \n",
    "4. Define Identity loss:  \n",
    "    - If you were to send an orange image to an orange-generator, ideally if the generator has understood everything about oranges, it should not change the image and should \"generate\" the exact same image. We thus create an identity using this knowledge. \n",
    "    - Identity loss should be minimal when an image of class A (real_A) is passed through G_BA and compared with real_A. \n",
    "    - Identity loss should be minimal when an image of class B (real_B) is passed through G_AB and compared with real_B.  \n",
    "\n",
    "\n",
    "\n",
    "5. Define GAN loss:  \n",
    "    - Discriminator and generator loss for real_A and fake_A (fake_A is obtained when real_B image is passed through G_BA)\n",
    "    - Discriminator and generator loss for real_B and fake_B (fake_B is obtained when the real_A image is passed through G_AB)  \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "6. Define re-cycle loss:\n",
    "    - Consider a scenario where an image of an apple is to be transformed by an orange-generator to generate a fake orange, and the fake orange is to be transformed back to an apple by the apple-generator.\n",
    "\n",
    "    - fake_B, which is the output when real_A is passed through G_AB, should regenerate real_A when fake_B is passed through G_BA.\n",
    "\n",
    "    - fake_A, which is the output when real_B is passed through G_BA, should regenerate real_B when fake_A is passed through G_AB.\n",
    "\n",
    "7. Optimize for the weighted loss of the three losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T17:33:32.551514Z",
     "start_time": "2022-07-23T17:33:28.290226Z"
    },
    "id": "lNYTA_siln-p",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!wget -q https://www.dropbox.com/s/2xltmolfbfharri/apples_oranges.zip\n",
    "!unzip -q apples_oranges.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T17:33:45.698070Z",
     "start_time": "2022-07-23T17:33:44.264717Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIJGnn38l2xM",
    "outputId": "c5fe279e-adf8-4339-c34b-68c2a07dce40",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install torch_snippets torch_summary\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from torch_snippets import *\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T18:30:01.696896Z",
     "start_time": "2022-07-23T18:30:01.690932Z"
    },
    "id": "BsSIvEHcl6r7",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(int(IMAGE_SIZE*1.33)),\n",
    "    transforms.RandomCrop((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T17:35:16.013893Z",
     "start_time": "2022-07-23T17:35:16.006712Z"
    },
    "id": "JN4bPxjYl8fM",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class CycleGANDataset(Dataset):\n",
    "    def __init__(self, apples, oranges):\n",
    "        self.apples = Glob(apples)\n",
    "        self.oranges = Glob(oranges)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        apple = self.apples[ix % len(self.apples)]\n",
    "        orange = choose(self.oranges)\n",
    "        apple = Image.open(apple).convert('RGB')\n",
    "        orange = Image.open(orange).convert('RGB')\n",
    "        return apple, orange\n",
    "\n",
    "    def __len__(self): return max(len(self.apples), len(self.oranges))\n",
    "    def choose(self): return self[randint(len(self))]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        srcs, trgs = list(zip(*batch))\n",
    "        srcs = torch.cat([transform(img)[None] for img in srcs], 0).to(device).float()\n",
    "        trgs = torch.cat([transform(img)[None] for img in trgs], 0).to(device).float()\n",
    "        return srcs.to(device), trgs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T17:35:37.328881Z",
     "start_time": "2022-07-23T17:35:37.303715Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-U3g9Gb8l_aa",
    "outputId": "73768266-5b9e-4937-9837-564b9aed1876",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "trn_ds = CycleGANDataset('apples_train', 'oranges_train')\n",
    "val_ds = CycleGANDataset('apples_test', 'oranges_test')\n",
    "\n",
    "trn_dl = DataLoader(trn_ds, batch_size=1, shuffle=True, collate_fn=trn_ds.collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size=5, shuffle=True, collate_fn=val_ds.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T18:29:45.071449Z",
     "start_time": "2022-07-23T18:29:45.064358Z"
    },
    "id": "ua6A14QWmA7s",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T17:37:23.699585Z",
     "start_time": "2022-07-23T17:37:23.692347Z"
    },
    "id": "Ur5TL0h7mFLY",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, num_residual_blocks=9):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "        out_features = 64\n",
    "        channels = 3\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "        self.apply(weights_init_normal)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T17:37:31.802745Z",
     "start_time": "2022-07-23T17:37:31.795788Z"
    },
    "id": "jL70jua7mHRR",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        channels, height, width = 3, IMAGE_SIZE, IMAGE_SIZE\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "        self.apply(weights_init_normal)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T17:38:20.442502Z",
     "start_time": "2022-07-23T17:38:20.436330Z"
    },
    "id": "NgPzlws1mJGU",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sample():\n",
    "    data = next(iter(val_dl))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    \n",
    "    real_A, real_B = data\n",
    "    fake_B = G_AB(real_A)\n",
    "    fake_A = G_BA(real_B)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
    "    show(image_grid.detach().cpu().permute(1,2,0).numpy(), sz=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T18:28:26.248879Z",
     "start_time": "2022-07-23T18:28:26.241707Z"
    },
    "id": "vbWP5fLPmLI5",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def generator_train_step(Gs, optimizer, real_A, real_B):\n",
    "    G_AB, G_BA = Gs\n",
    "    optimizer.zero_grad()\n",
    "    loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "    loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "\n",
    "    loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "    fake_B = G_AB(real_A)\n",
    "    loss_GAN_AB = criterion_GAN(D_B(fake_B), torch.Tensor(np.ones((len(real_A), 1, 16, 16))).to(device))\n",
    "    fake_A = G_BA(real_B)\n",
    "    loss_GAN_BA = criterion_GAN(D_A(fake_A), torch.Tensor(np.ones((len(real_A), 1, 16, 16))).to(device))\n",
    "\n",
    "    loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "    recov_A = G_BA(fake_B)\n",
    "    loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "    recov_B = G_AB(fake_A)\n",
    "    loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "    loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "    loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "    loss_G.backward()\n",
    "    optimizer.step()\n",
    "    return loss_G, loss_identity, loss_GAN, loss_cycle, loss_G, fake_A, fake_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T18:29:06.347489Z",
     "start_time": "2022-07-23T18:29:06.341852Z"
    },
    "id": "ja2h_bu4mWsU",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def discriminator_train_step(D, real_data, fake_data, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    loss_real = criterion_GAN(D(real_data), torch.Tensor(np.ones((len(real_data), 1, 16, 16))).to(device))\n",
    "    loss_fake = criterion_GAN(D(fake_data.detach()), torch.Tensor(np.zeros((len(real_data), 1, 16, 16))).to(device))\n",
    "    loss_D = (loss_real + loss_fake) / 2\n",
    "    loss_D.backward()\n",
    "    optimizer.step()\n",
    "    return loss_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T18:30:07.884094Z",
     "start_time": "2022-07-23T18:30:07.587830Z"
    },
    "id": "tbIQMqcpmYJx",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "G_AB = GeneratorResNet().to(device)\n",
    "G_BA = GeneratorResNet().to(device)\n",
    "D_A = Discriminator().to(device)\n",
    "D_B = Discriminator().to(device)\n",
    "\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=0.0002, betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "lambda_cyc, lambda_id = 10.0, 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-23T23:18:03.104283Z",
     "start_time": "2022-07-23T18:30:22.703789Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CQnri-dpmaCp",
    "outputId": "c0d03b6a-e3af-49e8-b43f-72fd17ecf478",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "log = Report(n_epochs)\n",
    "for epoch in range(n_epochs):\n",
    "    N = len(trn_dl)\n",
    "    for bx, batch in enumerate(trn_dl):\n",
    "        real_A, real_B = batch\n",
    "\n",
    "        loss_G, loss_identity, loss_GAN, loss_cycle, loss_G, fake_A, fake_B = generator_train_step((G_AB,G_BA), optimizer_G, real_A, real_B)\n",
    "        loss_D_A = discriminator_train_step(D_A, real_A, fake_A, optimizer_D_A)\n",
    "        loss_D_B = discriminator_train_step(D_B, real_B, fake_B, optimizer_D_B)\n",
    "        loss_D = (loss_D_A + loss_D_B) / 2\n",
    "        \n",
    "        log.record(epoch+(1+bx)/N, loss_D=loss_D.item(), loss_G=loss_G.item(), \n",
    "                   loss_GAN=loss_GAN.item(), loss_cycle=loss_cycle.item(), \n",
    "                   loss_identity=loss_identity.item(), end='\\r')\n",
    "        if bx%100==0: generate_sample()\n",
    "\n",
    "    log.report_avgs(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0muEWW0jmdZ8",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "CycleGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
